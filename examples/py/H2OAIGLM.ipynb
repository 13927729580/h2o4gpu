{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded H2OAIGLM CPU library\n",
      "\n",
      "Loaded H2OAIGLM GPU library.\n"
     ]
    }
   ],
   "source": [
    "#%reset -f\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "import time\n",
    "import h2oaiglm as h2oaiglm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data Frame and create raw X and y arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to read data via feather: 2.157763957977295\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "from os.path import expanduser\n",
    "home = str(expanduser(\"~\"))\n",
    "filepostfix=\"/h2oai-prototypes/glm-bench/ipums.feather\"\n",
    "df = feather.read_dataframe(home+filepostfix)\n",
    "#df = pd.read_csv(\"../R/data.csv\")\n",
    "t1 = time.time()\n",
    "print(\"Time to read data via feather: %r\" % (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.columns[-1] ## last column is the response\n",
    "cols = [c for c in df.columns if c != target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55776, 9732)\n",
      "(55776,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(df.ix[:,cols], order='f').astype('float32')\n",
    "y = np.array(df[target].values, dtype='float32')\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H2O AI GLM using the GPU Pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original m=55776 n=9732\n",
      "fortran=1\n",
      "Size of Train rows=44621 valid rows=11155\n",
      "mTrain=44621 mvalid=11154\n",
      "sdtrainY: 39926.0\n",
      "meantrainY: 32721.7\n",
      "sdvalidY: 42661.9\n",
      "meanvalidY: 33843.7\n",
      "lambda_max0: 7947.98718092\n",
      "New n=9733\n"
     ]
    }
   ],
   "source": [
    "nGPUs=4\n",
    "nlambda=100\n",
    "nalpha=16\n",
    "validFraction=0.2\n",
    "\n",
    "# set solver cpu/gpu according to input args\n",
    "if((nGPUs>0) and (h2oaiglm.ElasticNetSolverGPU is None)):\n",
    "    print(\"\\nGPU solver unavailable, using CPU solver\\n\")\n",
    "    nGPUs=0\n",
    "\n",
    "sharedA = 0\n",
    "sourceme = 0\n",
    "sourceDev = 0\n",
    "nThreads = 1 if(nGPUs==0) else nGPUs # not required number of threads, but normal.  Bit more optimal to use 2 threads for CPU, but 1 thread per GPU is optimal.\n",
    "intercept = 1\n",
    "standardize = 0\n",
    "lambda_min_ratio = 1e-7\n",
    "nLambdas = nlambda\n",
    "nAlphas = nalpha\n",
    "\n",
    "if standardize:\n",
    "    print (\"implement standardization transformer\")\n",
    "    exit()\n",
    "\n",
    "# Setup Train/validation Set Split\n",
    "morig = X.shape[0]\n",
    "norig = X.shape[1]\n",
    "print(\"Original m=%d n=%d\" % (morig,norig))\n",
    "fortran = X.flags.f_contiguous\n",
    "print(\"fortran=%d\" % (fortran))\n",
    "\n",
    "\n",
    "# Do train/valid split\n",
    "HO=int(validFraction*morig)\n",
    "H=morig-HO\n",
    "print(\"Size of Train rows=%d valid rows=%d\" % (H,HO))\n",
    "trainX = np.copy(X[0:H,:])\n",
    "trainY = np.copy(y[0:H])\n",
    "validX = np.copy(X[H:-1,:])\n",
    "validY = np.copy(y[H:-1])\n",
    "trainW = np.copy(trainY)*0.0 + 1.0 # constant unity weight\n",
    "\n",
    "mTrain = trainX.shape[0]\n",
    "mvalid = validX.shape[0]\n",
    "print(\"mTrain=%d mvalid=%d\" % (mTrain,mvalid))\n",
    "\n",
    "## TODO: compute these in C++ (CPU or GPU)\n",
    "sdtrainY = np.sqrt(np.var(trainY))\n",
    "print(\"sdtrainY: \" + str(sdtrainY))\n",
    "meantrainY = np.mean(trainY)\n",
    "print(\"meantrainY: \" + str(meantrainY))\n",
    "\n",
    "## TODO: compute these in C++ (CPU or GPU)\n",
    "sdvalidY = np.sqrt(np.var(validY))\n",
    "print(\"sdvalidY: \" + str(sdvalidY))\n",
    "meanvalidY = np.mean(validY)\n",
    "print(\"meanvalidY: \" + str(meanvalidY))\n",
    "\n",
    "## TODO: compute this in C++ (CPU or GPU)\n",
    "# compute without intercept column\n",
    "\n",
    "weights = 1./mTrain\n",
    "#weights = 1. # like current cpp driver\n",
    "if intercept==1:\n",
    "    lambda_max0 = weights * max(abs(trainX.T.dot(trainY-meantrainY)))\n",
    "else:\n",
    "    lambda_max0 = weights * max(abs(trainX.T.dot(trainY)))\n",
    "print(\"lambda_max0: \" + str(lambda_max0))\n",
    "\n",
    "if intercept==1:\n",
    "    trainX = np.hstack([trainX, np.ones((trainX.shape[0],1),dtype=trainX.dtype)])\n",
    "    validX = np.hstack([validX, np.ones((validX.shape[0],1),dtype=validX.dtype)])\n",
    "    n = trainX.shape[1]\n",
    "    print(\"New n=%d\" % (n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Solver\n",
      "Uploading\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "Detected np.float32\n",
      "c_void_p(1121219248128)\n",
      "c_void_p(1122958362112)\n",
      "c_void_p(1122959884288)\n",
      "c_void_p(1121213827584)\n",
      "c_void_p(1122957787136)\n",
      "Time to ingest data: 0.7800250053405762\n",
      "Solving\n",
      "single precision fit\n",
      "Done Solving\n",
      "Time to train H2O AI GLM: 119.09090614318848\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up Solver\")\n",
    "Solver = h2oaiglm.ElasticNetSolverGPU if(nGPUs>0) else h2oaiglm.ElasticNetSolverCPU\n",
    "#  Solver = h2oaiglm.ElasticNetSolverCPU\n",
    "assert Solver != None, \"Couldn't instantiate ElasticNetSolver\"\n",
    "enet = Solver(sharedA, nThreads, nGPUs, 'c' if fortran else 'r', intercept, standardize, lambda_min_ratio, nLambdas, nAlphas)\n",
    "\n",
    "\n",
    "## First, get backend pointers\n",
    "print(\"Uploading\")\n",
    "print(trainX.dtype)\n",
    "print(trainY.dtype)\n",
    "print(validX.dtype)\n",
    "print(validY.dtype)\n",
    "print(trainW.dtype)\n",
    "t0 = time.time()\n",
    "a,b,c,d,e = enet.upload_data(sourceDev, trainX, trainY, validX, validY, trainW)\n",
    "t1 = time.time()\n",
    "print(\"Time to ingest data: %r\" % (t1-t0))\n",
    "\n",
    "\n",
    "## Solve\n",
    "print(\"Solving\")\n",
    "t0 = time.time()\n",
    "enet.fit(sourceDev, mTrain, n, mvalid, intercept, standardize, a, b, c, d, e)\n",
    "t1 = time.time()\n",
    "print(\"Done Solving\")\n",
    "print(\"Time to train H2O AI GLM: %r\" % (t1-t0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
